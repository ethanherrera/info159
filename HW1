{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanherrera/info159/blob/main/HW1_Word_Embeddings.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# INFO 159/259\n",
        "#<center> Homework 1: Word Embeddings </center>\n",
        "\n",
        "<center> Due: February 4, 2025 @ 11:59pm </center>"
      ],
      "metadata": {
        "id": "qA-1YP6V82jp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Introduction"
      ],
      "metadata": {
        "id": "rMo-4cawj48b"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**MAKE A COPY OF THIS NOTEBOOK AND WORK ON THAT. If you do not work on a copy of this, your work will be erased.**"
      ],
      "metadata": {
        "id": "45wrQjCnfo4V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this homework, you will be working with word embeddings.\n",
        "\n",
        "In class, we've discussed [words](https://web.stanford.edu/~jurafsky/slp3/2.pdf) as dimensionality reduction, as well as the implications in choosing appropriate measures of **types** and **tokens**. In discussing words as singular units, and about the act of **tokenizing**, we've developed understanding of why the choice of delimiters can change the units and resulting findings.  We've discussed how the representation of words is informed by the *distributional hypothesis* (we understand the meaning of a word by the distribution of contexts in which it is used)."
      ],
      "metadata": {
        "id": "_2m9ihowioyw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As explored in [Jurafsky and Martin](https://web.stanford.edu/~jurafsky/slp3/6.pdf), word embeddings are learned vector representations of words. In this homework, we are going to work with dense word embedding models."
      ],
      "metadata": {
        "id": "Bzp37R0VjmKq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If you have never used Google Colab before, a [cheat sheet](https://colab.research.google.com/github/Tanu-N-Prabhu/Python/blob/master/Cheat_sheet_for_Google_Colab.ipynb#scrollTo=jwK2yGZZDeLH) may be found here.\n"
      ],
      "metadata": {
        "id": "JLDga610Uihf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "> In this homework, a CPU runtime will work, so don't waste any GPU resources you might have (which you'll want later this semester). Be sure the CPU runtime is active by going to: **Runtime > Change runtime rype > CPU**.\n",
        "\n"
      ],
      "metadata": {
        "id": "LlTK_ar43kQX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q0: Nearest Neighbors with Word Embeddings"
      ],
      "metadata": {
        "id": "4vM6yQqf-11_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have a learned representation of a text we are working with, we can use our model to understand the meaning of a word based on its neighbours. Semantically similar words have vector representations or **embeddings** that are similar, as described in [6.8](https://web.stanford.edu/~jurafsky/slp3/6.pdf). Let's look at this with a pre-trained model.\n",
        "\n"
      ],
      "metadata": {
        "id": "N7BXTBMmx0ez"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from gensim.models import KeyedVectors\n",
        "import operator\n",
        "from sklearn.decomposition import PCA"
      ],
      "metadata": {
        "id": "wxVYcF6QbxwC"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Set-up, Introducing GloVe"
      ],
      "metadata": {
        "id": "d43_6sB96xty"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to work with a pretrained [GloVe word embedding model](https://nlp.stanford.edu/projects/glove/) that has 100,000 words in its vocabulary and 100 dimensions."
      ],
      "metadata": {
        "id": "GkC5nqh0b-fk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/dbamman/nlp25/raw/main/data/glove.6B.100d.100K.w2v.txt"
      ],
      "metadata": {
        "id": "g6w0DbSDb8I1",
        "outputId": "111928fa-3f6e-43fe-f3f1-b156c33f3aee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-01-29 20:40:21--  https://github.com/dbamman/nlp25/raw/main/data/glove.6B.100d.100K.w2v.txt\n",
            "Resolving github.com (github.com)... 140.82.112.3\n",
            "Connecting to github.com (github.com)|140.82.112.3|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://raw.githubusercontent.com/dbamman/nlp25/main/data/glove.6B.100d.100K.w2v.txt [following]\n",
            "--2025-01-29 20:40:22--  https://raw.githubusercontent.com/dbamman/nlp25/main/data/glove.6B.100d.100K.w2v.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.109.133, 185.199.108.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.109.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 85952085 (82M) [text/plain]\n",
            "Saving to: ‘glove.6B.100d.100K.w2v.txt’\n",
            "\n",
            "glove.6B.100d.100K. 100%[===================>]  81.97M   108MB/s    in 0.8s    \n",
            "\n",
            "2025-01-29 20:40:24 (108 MB/s) - ‘glove.6B.100d.100K.w2v.txt’ saved [85952085/85952085]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "glove_100=KeyedVectors.load_word2vec_format(\"glove.6B.100d.100K.w2v.txt\", binary=False)\n",
        "print(\"Vocab size: \", len(glove_100.index_to_key))\n",
        "print(\"Dimension: \", glove_100.vector_size)"
      ],
      "metadata": {
        "id": "dP0DyzDccSHh",
        "outputId": "8154a2c7-7009-4f7f-c650-258dfe956b23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocab size:  100000\n",
            "Dimension:  100\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Each word is represented as a 100-dimensional vector in our model. We can access the vector for a given word by passing the word as a **key** to our KeyedVector word embedding model, which is simply a structure for accessing learned embeddings. This will return an array which we can process."
      ],
      "metadata": {
        "id": "B7cp6rY6c45z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "glove_100['burger']"
      ],
      "metadata": {
        "id": "-_O-i_PKc3RE",
        "outputId": "2faadc04-8706-4710-dd0c-036f472eda9e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0.33133 , -0.27067 , -0.13907 , -0.088709, -0.043864,  0.063834,\n",
              "        0.25138 , -0.091424, -0.1784  , -0.68607 , -0.21564 ,  0.18211 ,\n",
              "       -0.23288 ,  0.34244 ,  0.18765 ,  0.42714 ,  0.090041, -0.38193 ,\n",
              "       -0.86077 ,  0.10677 ,  0.97605 ,  0.33179 , -0.22349 , -0.6037  ,\n",
              "        0.3639  ,  1.1531  ,  0.57479 , -0.26689 , -0.2192  ,  0.27449 ,\n",
              "        0.58477 ,  0.72972 ,  0.55298 , -0.54246 ,  0.41626 ,  0.80416 ,\n",
              "        0.016013,  0.12076 , -0.14406 , -1.3187  ,  1.0801  ,  0.072487,\n",
              "       -0.151   , -0.84384 ,  0.072104,  0.80528 , -0.5371  , -0.58189 ,\n",
              "        0.2129  ,  0.19258 ,  0.022321, -0.21964 , -0.90854 ,  0.033854,\n",
              "       -0.74511 , -0.71059 , -0.40855 ,  0.64035 ,  0.1504  , -0.036281,\n",
              "        0.18085 , -0.47587 ,  0.90079 ,  0.12859 ,  0.16879 , -0.47241 ,\n",
              "       -0.49883 ,  0.57205 , -0.30949 ,  0.088298, -0.13453 ,  0.16067 ,\n",
              "        0.26489 ,  0.097569, -0.32744 , -0.26541 , -0.89286 ,  0.1455  ,\n",
              "       -0.079116,  0.78621 ,  0.23043 , -0.48585 , -0.066305,  0.62033 ,\n",
              "       -0.3532  , -0.80811 , -0.42646 , -0.6372  , -0.51965 , -0.29796 ,\n",
              "        0.43657 ,  0.19157 ,  0.059287, -0.13863 , -0.50692 , -0.3477  ,\n",
              "        0.46988 , -0.22711 ,  0.58617 ,  0.28682 ], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. Cosine Similarity\n"
      ],
      "metadata": {
        "id": "gbZ19e8zddZe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are now going to write a function to retreive the top-k similar words to a given word from our pre-trained word embedding model. We are going to use the **cosine similarity** of the vector representation of words to calculate the semantic similarity between words.\n",
        "\n",
        "$$\n",
        "cosine\\_similarity(A,B)= (A \\cdot B) \\div (||A|| \\times ||B||)\n",
        "$$"
      ],
      "metadata": {
        "id": "DKK1-33xd_uu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def cosine_similarity(vector1, vector2):\n",
        "  dot_product=np.dot(vector1, vector2)\n",
        "  maginitude_vector1=np.linalg.norm(vector1)\n",
        "  maginitude_vector2=np.linalg.norm(vector2)\n",
        "  cosine= dot_product / (maginitude_vector1 * maginitude_vector2)\n",
        "  return cosine"
      ],
      "metadata": {
        "id": "8pl79Vqcdcib"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(glove_100['burger'], glove_100['fries'])"
      ],
      "metadata": {
        "id": "hEsEBLDJeyKT",
        "outputId": "19d92cca-a0b8-43b4-f724-60c0f8548641",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.6424609"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cosine_similarity(glove_100['burger'], glove_100['pen'])"
      ],
      "metadata": {
        "id": "Y6Yi3w0we6ZC",
        "outputId": "8b2d9e36-09b1-42f2-94f2-ba25b4054e66",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.15541728"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we would expect, the cosine similarity between the words 'burger' and 'fries' is higher than the cosine similarity between the words 'burger' and 'pen'."
      ],
      "metadata": {
        "id": "wBXxnj_0fhpQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### B. Top-k nearest neighbors"
      ],
      "metadata": {
        "id": "-4amguGG6UxJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "We will now use this function to score the similarity between our target word and all other words in our vocabulary so we can find the top-k most similar words."
      ],
      "metadata": {
        "id": "s3_wIMFQ6lA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_k_nearest_neighbors(model, query, k):\n",
        "  similarity_scores={}\n",
        "  for word in model.index_to_key:\n",
        "    if word!=query:                   # cosine similarity of a word to itself is 1; it will always be the most similar to itself\n",
        "      similarity_scores[word]=cosine_similarity(model[word], model[query])\n",
        "  sorted_score = sorted(similarity_scores.items(), key=operator.itemgetter(1), reverse=True)\n",
        "  for idx, (k, v) in enumerate(sorted_score[:k]):\n",
        "            print(\"%s\\t%s\\t%.5f\" % (idx,k,v))"
      ],
      "metadata": {
        "id": "9B8lBusafABj"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "find_k_nearest_neighbors(glove_100, 'burger', 5)"
      ],
      "metadata": {
        "id": "_8UJyVLtf3Hw",
        "outputId": "436e4a59-6a9f-48df-bbdd-7ca6a72758d6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tburgers\t0.70148\n",
            "1\tpizza\t0.69776\n",
            "2\tkfc\t0.66453\n",
            "3\ttaco\t0.66316\n",
            "4\tmcdonald\t0.66145\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "find_k_nearest_neighbors(glove_100, 'pen', 5)"
      ],
      "metadata": {
        "id": "aCBtxiQ8f3t_",
        "outputId": "09a5ccfc-b016-4dd9-ce5d-e75a62af7d9c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\tpencil\t0.61015\n",
            "1\tballpoint\t0.60232\n",
            "2\tpens\t0.60200\n",
            "3\tle\t0.55509\n",
            "4\tink\t0.52271\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can try different values of k and different queries to explore the learned representation in the provided pre-trained model.\n",
        "\n",
        "We want you to get familiar with word embeddings and how you can manipluate the vectors in the models."
      ],
      "metadata": {
        "id": "_90TDljmiNQu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "**Deliverable 1**\n"
      ],
      "metadata": {
        "id": "H78c_q390WYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Which term (by cosine similarity over glove vectors) is most similar to \"burger\"?\n",
        "\n",
        "<ol type = A>\n",
        "\n",
        "<li>pencil</li>\n",
        "<li>car</li>\n",
        "<li>movie</li>\n",
        "\n",
        "</ol>\n",
        "\n",
        "Fill in your answer in the code block below; your answer should be either \"A\", \"B\" or \"C\"\n"
      ],
      "metadata": {
        "id": "nLvMcHId6oYm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "terms = [\"pencil\", \"car\", \"movie\"]\n",
        "result = []\n",
        "for term in terms:\n",
        "  sim = cosine_similarity(glove_100['burger'], glove_100[term])\n",
        "  result.append((term, sim))\n",
        "print(result)\n",
        "Q0 = \"C\""
      ],
      "metadata": {
        "id": "X61mEA8KFHGe",
        "outputId": "85c9631b-3fff-4f15-f1b4-65574f6f9fc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[('pencil', 0.0034915318), ('car', 0.1262063), ('movie', 0.27131867)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Q1: Implementation of SemAxis"
      ],
      "metadata": {
        "id": "mnuWRdMkhPxq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As mentioned in class, [SemAxis](https://arxiv.org/pdf/1806.05521.pdf) is a method for scoring terms along a user-defined axis.  In this queston, you will be exploring vector semantics (as explained in [6.2](https://web.stanford.edu/~jurafsky/slp3/6.pdf)) and implementing this method.\n",
        "\n",
        "Given a set of word embeddings for positive terms $S^+ = \\{v_1^+, \\ldots v_n^+\\}$ and embeddings for negative terms $S^- = \\{v_1^-, \\ldots v_n^-\\}$ that define the endpoints of the axis, a semantic axis is given as:\n",
        "\n",
        "$$\n",
        "\\mathbf{V}^+ = {1 \\over n} \\sum_1^n v_i^+\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{V}^- = {1 \\over m} \\sum_1^m v_i^-\n",
        "$$\n",
        "\n",
        "$$\n",
        "\\mathbf{V}_{\\textrm{axis}} = \\mathbf{V}^+ - \\mathbf{V}^-\n",
        "$$"
      ],
      "metadata": {
        "id": "Fu9QQ4K1HGJ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A: Implement `get_semaxis`\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "R3S70v199UPG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deliverable 2**\n",
        "\n",
        "Using the information above, implement the function `get_semaxis`, which will take in a set of glove embeddings (`vectors`, a `KeyedVectors` object) as well as two lists of the positive and negative terms which define the endpoints of the axis. The function should return $\\mathbf{V}_{\\textrm{axis}}$, in the form of an `numpy.array`."
      ],
      "metadata": {
        "id": "_sIIHaY_D3-d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_semaxis(vectors, positive_terms, negative_terms):\n",
        "    '''\n",
        "    vectors: KeyedVectors word embeddings\n",
        "    positive_terms: list of terms (strings) defining one end of an axis\n",
        "    negative_terms: list of terms (strings) defining the other end of an axis\n",
        "\n",
        "    output: SemAxis vector (a numpy array)\n",
        "    '''\n",
        "    pos_vectors = [vectors[term] for term in positive_terms if term in vectors]\n",
        "    V_plus = np.mean(pos_vectors, axis=0) if pos_vectors else np.zeros(vectors.vector_size)\n",
        "\n",
        "    neg_vectors = [vectors[term] for term in negative_terms if term in vectors]\n",
        "    V_minus = np.mean(neg_vectors, axis=0) if neg_vectors else np.zeros(vectors.vector_size)\n",
        "\n",
        "    V_axis = V_plus - V_minus\n",
        "    return V_axis"
      ],
      "metadata": {
        "id": "vuPFjLRBFV84"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "#### B: Implement `get_semaxis_score`\n"
      ],
      "metadata": {
        "id": "BV44nqu18Was"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Deliverable 3**\n",
        "\n",
        "Implement the function `get_semaxis_score` to calculate the score of a word along the specified semantic axis. See section 3.1.3 from the SemAxis paper for details, but this score is simply the cosine similarity between the term and the axis:\n",
        "\n",
        "$$\n",
        "score(w)_{\\mathbf{V_\\textrm{axis}}} = \\textrm{cos}(v_w, \\mathbf{V}_\\textrm{axis})\n",
        "$$\n",
        "\n",
        "`get_semaxis_score` takes in a `semaxis` (the output of your `get_semaxis` function above) and a target word's vector. The function should return the score of where the target would land on the axis, in the form of a `float`."
      ],
      "metadata": {
        "id": "jUvop_5rD85Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_semaxis_score(axis, target_vector):\n",
        "    '''\n",
        "    axis: SemAxis vector\n",
        "    target_vector: target term's vector\n",
        "\n",
        "    output: target vector's score on SemAxis (a float)\n",
        "    '''\n",
        "\n",
        "    # Compute cosine similarity\n",
        "    dot_product = np.dot(axis, target_vector)\n",
        "    norm_axis = np.linalg.norm(axis)\n",
        "    norm_target = np.linalg.norm(target_vector)\n",
        "\n",
        "    # Avoid division by zero (e.g., if axis is a zero vector)\n",
        "    if norm_axis == 0 or norm_target == 0:\n",
        "        return 0.0\n",
        "\n",
        "    score = dot_product / (norm_axis * norm_target)\n",
        "    return float(score)"
      ],
      "metadata": {
        "id": "WwltlZ3Fs6HU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, we are using \"woman\" and \"women\" as defining one end of an axis, and \"man\" and \"men\" as defining the other."
      ],
      "metadata": {
        "id": "nR3yh--k-nju"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# DO NOT CHANGE THIS CELL\n",
        "axis=get_semaxis(glove_100, [\"woman\", \"women\"], [\"man\", \"men\"])\n",
        "target_vector=glove_100['actress']\n",
        "get_semaxis_score(axis, target_vector)"
      ],
      "metadata": {
        "id": "r6S4VY9OJqBh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14c47d43-efdf-4356-f285-082b97225886"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.3424988090991974"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "Now let's score a set of target terms along that axis. (*Just run the following, do not change.*)"
      ],
      "metadata": {
        "id": "qMwNMJtbY0sy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def score_list_of_targets(vectors, positive_terms=None, negative_terms=None, target_words=None):\n",
        "    scores=[]\n",
        "\n",
        "    axis=get_semaxis(vectors, positive_terms, negative_terms)\n",
        "\n",
        "    for target in target_words:\n",
        "        scores.append((get_semaxis_score(axis, glove_100[target]), target))\n",
        "\n",
        "    for k,v in reversed(sorted(scores)):\n",
        "        print(\"%.3f\\t%s\" % (k,v))"
      ],
      "metadata": {
        "id": "rFdoFchLYw4M"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets=[\"doctor\", \"nurse\", \"actor\", \"actress\", \"mechanic\", \"librarian\", \"architect\", \"magician\", \"cook\", \"chef\"]"
      ],
      "metadata": {
        "id": "8e557-O7Y5tQ"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_list_of_targets(glove_100, positive_terms=[\"woman\", \"women\"], negative_terms=[\"man\", \"men\"], target_words=targets)"
      ],
      "metadata": {
        "id": "k6n88K38Y9a2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7582a8a3-fd06-4832-e410-e3384319e408"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.342\tactress\n",
            "0.294\tnurse\n",
            "0.219\tlibrarian\n",
            "0.106\tdoctor\n",
            "0.024\tactor\n",
            "0.003\tchef\n",
            "-0.019\tcook\n",
            "-0.075\tarchitect\n",
            "-0.153\tmagician\n",
            "-0.194\tmechanic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Consider what these scores mean in context. What does having a larger score versus a lower one mean in context, given our SemAxis?  (Just reflect on this question yourself -- there is no deliverable for this reflection.)"
      ],
      "metadata": {
        "id": "Dm3QBsyE-x18"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Q2: Debiasing"
      ],
      "metadata": {
        "id": "Pnsg0Z6UhSse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implicit bias and prejudice that are present within the training data end up as biases within word embeddings, as described in [6.11 and 6.12](https://web.stanford.edu/~jurafsky/slp3/6.pdf).\n",
        "One potential method of debiasing is presented within [Bolukbasi et al. 2016](https://arxiv.org/pdf/1607.06520). We will be exploring this method, as interpreted through [Vargas and Cotterell 2024](https://arxiv.org/abs/2009.09435)."
      ],
      "metadata": {
        "id": "0U9VHFcmrIPR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this formulation, debiasing involves the following 3 steps:\n",
        "\n",
        "<ol type = A>\n",
        "\n",
        "<li>creating \"defining sets\" which define the subspaces</li>\n",
        "<li>get subspace axis through PCA</li>\n",
        "<li>subtracting orthogonal projection onto that subspace from the original embeddings</li>\n",
        "\n",
        "</ol>"
      ],
      "metadata": {
        "id": "GSQ2Dt-GgQHh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### A. Defining sets"
      ],
      "metadata": {
        "id": "oqoAjdbaQfCM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this exercise, let's define a gender subspace as our \"defining sets\" as what is below.\n",
        "\n",
        "$$\n",
        "D_1 = \\{man, woman\\}\\\\\n",
        "D_2 = \\{mr., mrs.\\}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "dfXO4j-Agu0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this example, following [Vargas and Cotterell 2024](https://arxiv.org/abs/2009.09435), we will be using these key terms in order to define and create a gender subspace through the creation of a new matrix $W$.\n"
      ],
      "metadata": {
        "id": "hdfaD0uKipKd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Building and interpreting defining sets**\n",
        "\n",
        "\n",
        "Using $e_{word}$ to denote the embedding for a word, this matrix should be made up of each defining term's embedding\n",
        "\n",
        "> in $D_1$ our defining terms's embeddings would be $e_{man}, e_{woman}$\n",
        "\n",
        "being subtracted by the average of the embeddings within the given set.\n",
        "\n",
        "\n",
        "> $e_{man} - \\textrm{mean}(e_{man},e_{woman})$ \\\n",
        "> $e_{woman} - \\textrm{mean}(e_{man},e_{woman})$"
      ],
      "metadata": {
        "id": "Onr-KV87ld2l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This results in the following matrix given both the defining sets ($D_1, D_2$) above:\n",
        "\n",
        "$$\n",
        "W=\n",
        "\\begin{bmatrix}\n",
        "e_{man} - \\textrm{mean}(e_{man},e_{woman}) \\\\\n",
        "e_{woman} - \\textrm{mean}(e_{man},e_{woman})\\\\\n",
        "e_{mr.} - \\textrm{mean}(e_{mr.},e_{mrs.})\\\\\n",
        "e_{mrs.} - \\textrm{mean}(e_{mr.},e_{mrs.})\\\\\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Deliverable 4**\n",
        "\n",
        "Now, complete the `create_design_matrix` function below to construct this design matrix $W$ and return it. This function should output a single numpy.array of size $4 \\times 100$, given our embeddings have 100 dimensions (as noted in Q0)"
      ],
      "metadata": {
        "id": "dbFbjKVnk-gA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_design_matrix(vectors, D1, D2):\n",
        "    '''\n",
        "    vectors: KeyedVectors word embeddings\n",
        "    D1: list of terms (strings) comprises the first defining set\n",
        "    D2: list of terms (strings) comprises the second defining set\n",
        "\n",
        "    output: a numpy.array\n",
        "\n",
        "    '''\n",
        "    design_vectors = []\n",
        "\n",
        "    valid_D1 = [term for term in D1 if term in vectors]\n",
        "    if valid_D1:\n",
        "        embeddings_D1 = [vectors[term] for term in valid_D1]\n",
        "        mean_D1 = np.mean(embeddings_D1, axis=0)\n",
        "        for term in valid_D1:\n",
        "            centered_vec = vectors[term] - mean_D1\n",
        "            design_vectors.append(centered_vec)\n",
        "\n",
        "    valid_D2 = [term for term in D2 if term in vectors]\n",
        "    if valid_D2:\n",
        "        embeddings_D2 = [vectors[term] for term in valid_D2]\n",
        "        mean_D2 = np.mean(embeddings_D2, axis=0)\n",
        "        for term in valid_D2:\n",
        "            centered_vec = vectors[term] - mean_D2\n",
        "            design_vectors.append(centered_vec)\n",
        "\n",
        "    return np.array(design_vectors) if design_vectors else np.zeros((0, vectors.vector_size))"
      ],
      "metadata": {
        "id": "xg0JlQDAHHi8"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "design_matrix=create_design_matrix(glove_100, [\"man\", \"woman\"], [\"mr.\", \"mrs.\"])"
      ],
      "metadata": {
        "id": "GZbcn6pwx3LP"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### B. PCA\n",
        "\n",
        "Our next step is to find the bias subspace that exists within this data -- i.e., the vector that corresponds to the e.g. \"gender\" information encoded within it. As [Vargas and Cotterell 2024](https://arxiv.org/abs/2009.09435) note, the bias subspace is the first principle component of that matrix $W$, found by running [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) on it."
      ],
      "metadata": {
        "id": "n2Toj_AaQqR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Example**\n",
        "\n",
        "As a refresher, here's how you run [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) on a random matrix to get the *first principle component*."
      ],
      "metadata": {
        "id": "54mGzDe7nQM6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fake_matrix=np.random.rand(3,3)\n",
        "print(\"Fake matrix:\")\n",
        "print(fake_matrix)"
      ],
      "metadata": {
        "id": "8YHHHAXAnvgK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4607f28c-067b-4dae-cf3e-7937cee8e066"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake matrix:\n",
            "[[0.14693552 0.28811868 0.25510548]\n",
            " [0.1555793  0.12755559 0.68452295]\n",
            " [0.25365807 0.89748118 0.93721023]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now that we have a random matrix, we will run PCA on it in order to extract the first principle component."
      ],
      "metadata": {
        "id": "A6YTzc5dqSPb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca=PCA(n_components=1).fit(fake_matrix)        # Set n_components to number of principle components desired (1)\n",
        "pca_subspace=pca.components_[0]                 # Extract first principle component\n",
        "\n",
        "print(\"First principle component:\")\n",
        "print(pca_subspace)"
      ],
      "metadata": {
        "id": "ab10JOxhnyiA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf0473b2-5fa9-4c71-be9a-d7290fb04898"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "First principle component:\n",
            "[0.12109607 0.78427696 0.60847793]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# You'll see that this subspace is already normalized to unit length:\n",
        "print(pca_subspace)\n",
        "print(pca_subspace/np.sqrt(np.dot(pca_subspace, pca_subspace)))"
      ],
      "metadata": {
        "id": "VhdVsljJoMJj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e60ec842-9a00-422f-918b-6cf795950858"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.12109607 0.78427696 0.60847793]\n",
            "[0.12109607 0.78427696 0.60847793]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, run [PCA](https://en.wikipedia.org/wiki/Principal_component_analysis) over the `design_matrix`, because the *gender subspace is the first principle component* of that process.\n"
      ],
      "metadata": {
        "id": "S0q87J2rnqbc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pca=PCA(n_components=1).fit(design_matrix)\n",
        "subspace=pca.components_[0]"
      ],
      "metadata": {
        "id": "V9-uc--zIVNe"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### C. Debias"
      ],
      "metadata": {
        "id": "Xp4hLT6RQwTB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To debias a vector with respect to a bias subspace, we need to subtract the information in the bias subspace from it. We do so by first finding the orthogonal projection of the original vector onto that subspace.\n",
        "\n",
        "We find the orthogoal projection of any unit-normalized vector $w$ onto a subspace $b$ by:\n",
        "\n",
        "$$\n",
        "w_b = \\textrm{dot}(w,b) \\; b\n",
        "$$\n",
        "\n",
        "If $b$ and $x$ are 100 dimensions, $w_b$ is 100 dimensions too.\n",
        "\n",
        "Debiasing a vector is simply removing the information about that subspace from it. The debiased vector $w_d$ is then simply $w - w_b$.  "
      ],
      "metadata": {
        "id": "6WJkvTrYqepl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "**Deliverable 5**\n",
        "\n",
        "Implement the `debias` function below, which should debias any input vector `vec` with respect to the input subspace `subspace`."
      ],
      "metadata": {
        "id": "HhKkMqNaqp4c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "*Note: glove embeddings are not normalized ahead of time, so be sure to normalize them before carrying out your projection. Vector $v$ may be normalized by the following:*\n",
        "\n",
        "$$v  \\div \\sqrt{\\textrm{dot}(v,v)}$$"
      ],
      "metadata": {
        "id": "O-eaVO7xsaam"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def debias(vec, subspace):\n",
        "    '''\n",
        "    vec: vector to debias (numpy.array)\n",
        "    subspace: bias subspace (the first principle component of PCA on the design matrix $W$) (numpy.array)\n",
        "\n",
        "    output: a debiased vector (numpy.array )\n",
        "\n",
        "    '''\n",
        "    # Normalize the input vector to unit length\n",
        "    vec_norm = vec / np.linalg.norm(vec)\n",
        "\n",
        "    def project_onto_subspace(v, subspace):\n",
        "        # Compute scalar projection (dot product) and vector projection\n",
        "        scalar = np.dot(v, subspace)\n",
        "        projection = scalar * subspace\n",
        "        return projection\n",
        "\n",
        "    # Get the projection of the normalized vector onto the subspace\n",
        "    projected_vec = project_onto_subspace(vec_norm, subspace)\n",
        "\n",
        "    # Subtract the projection from the normalized vector to debias\n",
        "    debiased = vec_norm - projected_vec\n",
        "\n",
        "    return debiased"
      ],
      "metadata": {
        "id": "k4iXaLpRI-60"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Debias the vectors for the targets used above and see if debiasing changes the association between these terms and gender semaxis used above. *(You can simply run the code from this point forward.)*"
      ],
      "metadata": {
        "id": "4QiU4RmrrFOo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a reminder, let's see the original scores and  ranking of our targets."
      ],
      "metadata": {
        "id": "Tpzxp7-uAXGp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "score_list_of_targets(glove_100, positive_terms=[\"woman\", \"women\"], negative_terms=[\"man\", \"men\"], target_words=targets)"
      ],
      "metadata": {
        "id": "27f4ap1KAer7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "98829d27-dd92-4c0f-d3be-ed5ccb9e4087"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.342\tactress\n",
            "0.294\tnurse\n",
            "0.219\tlibrarian\n",
            "0.106\tdoctor\n",
            "0.024\tactor\n",
            "0.003\tchef\n",
            "-0.019\tcook\n",
            "-0.075\tarchitect\n",
            "-0.153\tmagician\n",
            "-0.194\tmechanic\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's see the new rankings of the same targets, after they were debiased."
      ],
      "metadata": {
        "id": "IezuOOkkA8vU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "diffs={}\n",
        "for term in targets:\n",
        "\n",
        "    m,w=glove_100.cosine_similarities(debias(glove_100[term], subspace), [debias(glove_100[\"man\"], subspace), debias(glove_100[\"woman\"], subspace)])\n",
        "    diffs[term]=w-m\n",
        "\n",
        "for k, v in sorted(diffs.items(), key=lambda item: item[1], reverse=True):\n",
        "    print(\"%.3f\\t%s\" % (v,k))"
      ],
      "metadata": {
        "id": "3y-UbXuNq6d0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "336eeda9-6b6f-4fa6-c356-f292f49b80f9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.043\tdoctor\n",
            "0.041\tactress\n",
            "0.039\tchef\n",
            "0.035\tlibrarian\n",
            "0.021\tnurse\n",
            "0.018\tactor\n",
            "0.015\tmagician\n",
            "-0.019\tarchitect\n",
            "-0.019\tmechanic\n",
            "-0.033\tcook\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take note of the differences in these scores, and what they may mean in terms of the influence of a biased dataset."
      ],
      "metadata": {
        "id": "B7lFprBprUT_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Closing and Submission"
      ],
      "metadata": {
        "id": "hNqoNRyHYSXB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Congratulations on finishing HW1! Please ensure that you submit the completed notebook (`.ipynb`) onto Gradescope before February 4 at 11:59pm.  The notebook you upload to Gradescope must be named **HW1.ipynb**.\n",
        "\n",
        "`File` --> `Download` --> `Download .ipynb`"
      ],
      "metadata": {
        "id": "xksFf9OHEFNv"
      }
    }
  ]
}
