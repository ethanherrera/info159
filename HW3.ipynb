{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanherrera/info159/blob/main/HW3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 3: Large Language Models & Prompting\n",
        "\n",
        "### Due Date: March 6, 2025 (11:59pm)\n",
        "\n",
        "**Total Points: 100 points**\n",
        "\n",
        "**Make a copy** of this notebook and work only on your copy, so that your work is retained. Make sure your copy is named `HW3.ipynb`.  \n",
        "\n",
        "As noted on the syllabus, this is one of the assignments that may require [Colab Pro](https://colab.research.google.com/signup) access (depending on your Colab usage already) with a personal Google account (your Berkeley institutional account may restrict setting up Google payments). We recommend that you start this assignment early in case you run into any GPU limit issues.\n",
        "\n",
        "Now, go to Runtime -> change runtime type -> select L4 GPU.\n",
        "\n",
        "**WARNING**: Remember to *close* this assignment's Colab tab when you are not working on it, as GPU resources are not unlimited.\n",
        "\n",
        "A guide for Colab GPU use [here](https://colab.research.google.com/notebooks/pro.ipynb).\n",
        "\n",
        "# Introduction\n",
        "\n",
        "In this assignment, you will work with an instruction-tuned language model, Phi 3.5 mini.\n",
        "\n",
        "We'll cover a few prompting strategies:\n",
        "  - Few-shot prompting\n",
        "  - Chain-of-thought\n",
        "  - Self-consistency\n",
        "\n",
        "We'll conclude by showing how one can thread multiple prompts into a conversation.\n",
        "\n",
        "**Only** modify code in between `### BEGIN SOLUTION` and `### END SOLUTION` lines. There are also some \"Think to yourself\" questions throughout the notebook; you do not need to answer these to earn homework points."
      ],
      "metadata": {
        "id": "NemnnBS4OXf9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install flash-attn --no-build-isolation\n",
        "!pip install datasets"
      ],
      "metadata": {
        "id": "k9Frp-3Nzkhs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from datasets import load_dataset, Dataset\n",
        "from tqdm.auto import tqdm\n",
        "from transformers.pipelines.pt_utils import KeyDataset\n",
        "import random\n",
        "from collections import Counter"
      ],
      "metadata": {
        "id": "9TO1FLYFIeMV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup & Dataset (5 points)\n",
        "\n",
        "Our data consists of **math problems**.\n",
        "\n",
        "Download our dataset:"
      ],
      "metadata": {
        "id": "ikH4BUKGMszE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/lucy3/temp/main/challenge.jsonl\n",
        "!wget https://raw.githubusercontent.com/lucy3/temp/main/math.jsonl\n",
        "!wget https://raw.githubusercontent.com/lucy3/temp/main/extra_exemplars.jsonl\n",
        "!wget https://raw.githubusercontent.com/lucy3/temp/main/exemplars.jsonl"
      ],
      "metadata": {
        "id": "tso06nOCK-7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A good rule of thumb when starting out with any modeling task is to take a look at the data you'll be working with.\n",
        "\n",
        "Our data is stored in in `.jsonl` files, where where each line of each file is one task example, or math problem, in `json` format.\n",
        "\n",
        "Note that the number of examples we'll work with is usually too small for sufficiently confident evaluation. For this homework assignment, we keep our data small to avoid excessive time and compute resource usage."
      ],
      "metadata": {
        "id": "-7YB9_fT966D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_files = {\"math\": \"math.jsonl\", \"challenge\": \"challenge.jsonl\"}\n",
        "dataset = load_dataset(\"json\", data_files=data_files)\n",
        "dataset"
      ],
      "metadata": {
        "id": "f9JePcN1sesb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task:\n",
        "- (5 points) Take a look at the data by printing out **one** dataset example from the `math` split of `dataset`."
      ],
      "metadata": {
        "id": "WUwk_dLtZdRW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Dataset example:\")\n",
        "# YOUR TASK: assign one dataset example to \"example\" variable\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "### END SOLUTION\n",
        "print(json.dumps(example, indent=4)) # the indent parameter here makes the json output pretty"
      ],
      "metadata": {
        "id": "FUJtItMc96Vm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The output of the cell above should have a similar structure as the snippet below, but may differ in content:\n",
        "\n",
        "```\n",
        "Dataset example:\n",
        "{\n",
        "  \"question\": \"Natalia sold clips to 48 of her friends in April, and then she sold half as many clips in May. How many clips did Natalia sell altogether in April and May?\",\n",
        "  \"answer\": \"72\"\n",
        "}\n",
        "```\n",
        "\n",
        "💡 Think to yourself: what do you notice about the math problems in our dataset? How difficult do they seem to be? What kinds of reasoning steps do they require?"
      ],
      "metadata": {
        "id": "BcHOfjRi-XhJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, download and load in the model we'll be working with.\n",
        "\n",
        "Phi 3.5 mini is a 3.8B parameter language model trained by Microsoft on synthetic data and the web. To learn more, see its [model report](https://arxiv.org/abs/2404.14219)."
      ],
      "metadata": {
        "id": "x-N30cxvraX1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hhkqJWtyjPYq"
      },
      "outputs": [],
      "source": [
        "torch.random.manual_seed(0)\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    \"microsoft/Phi-3.5-mini-instruct\", # model id from Hugging Face\n",
        "    device_map=\"cuda\", # use a GPU\n",
        "    torch_dtype=\"auto\", # automatically derive data type from the model’s weights\n",
        "    trust_remote_code=True,\n",
        "    attn_implementation='flash_attention_2', # attention algorithm for mitigating self-attention memory bottleneck\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3.5-mini-instruct\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Zero-shot Prompting (20 points)"
      ],
      "metadata": {
        "id": "xBTSCpmRrRdh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "First, we start with the following short prompt:\n"
      ],
      "metadata": {
        "id": "lYV0y-x7xmqY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zero_shot_prompt = \"Question: {question}\\nAnswer:\"\n",
        "print(zero_shot_prompt)"
      ],
      "metadata": {
        "id": "vBVA7AyPrWD4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's run our model on the above prompt.\n",
        "\n",
        "Your coding task in `format_chat_prompt` (20 points):\n",
        "- Insert the dataset `example`'s question into `prompt_template` to create the user prompt. Assume any prompt template contains a `{question}` slot, like `zero_shot_prompt` does above. Hint: use `.format()`\n",
        "- Then, create the language model's input `messages`, which includes the system prompt `\"You are a helpful assistant; follow the instructions in the prompt.\"` and user prompt.\n",
        "\n",
        "To learn about chat prompt templating, read this [guide](https://huggingface.co/docs/transformers/main/en/chat_templating) or Phi 3.5's [model card](https://huggingface.co/microsoft/Phi-3.5-mini-instruct)."
      ],
      "metadata": {
        "id": "71FZ-v_TxYzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def format_chat_prompt(dataset_split, prompt_template, n_examples=-1):\n",
        "  '''\n",
        "  @inputs:\n",
        "  - dataset_split: a string representing the split of the dataset to run the model on\n",
        "  - prompt_template: the prompt template for the user prompt.\n",
        "  - n_examples: an integer noting the number of examples to run the model. If -1, run the model on all examples\n",
        "  '''\n",
        "  all_messages = []\n",
        "  for i, example in enumerate(dataset[dataset_split]):\n",
        "    # if we have enough examples, stop\n",
        "    if n_examples > 0 and i == n_examples:\n",
        "      break\n",
        "\n",
        "    messages = []\n",
        "\n",
        "    # YOUR TASK: format task example into chat input format\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "    ### END SOLUTION\n",
        "\n",
        "    all_messages.append(messages)\n",
        "  return all_messages\n",
        "\n",
        "def run_one_by_one(pipe, all_messages, generation_args, dataset_split):\n",
        "  '''\n",
        "  Runs each example one by one, and prints out their outputs. Best for use with\n",
        "  a few examples for iterative prompt engineering.\n",
        "\n",
        "  @inputs:\n",
        "  - pipe: a transformer pipeline\n",
        "  - all_messages: a list of chat-formatted \"messages\"\n",
        "  - generation_args: parameters for running pipe\n",
        "  '''\n",
        "  predictions = []\n",
        "  i = 0\n",
        "  for messages in all_messages:\n",
        "    output = pipe(messages, **generation_args)\n",
        "    prompt = all_messages[i][1]['content'].strip()\n",
        "    response = output[0]['generated_text'].strip()\n",
        "\n",
        "    # populate the output dictionary\n",
        "    d = {}\n",
        "    example = dataset[dataset_split][i]\n",
        "    d['question'] = example['question']\n",
        "    d['answer'] = example['answer']\n",
        "    d['prediction'] = response\n",
        "    d['prompt'] = prompt\n",
        "    predictions.append(d)\n",
        "\n",
        "    print()\n",
        "    print(\"---- INPUT \" + str(i) + \" ----\")\n",
        "    print(tokenizer.apply_chat_template(messages, tokenize=False))\n",
        "    print(\"---- MODEL RESPONSE ----\")\n",
        "    print(response)\n",
        "    print(\"---- GROUND TRUTH ----\")\n",
        "    print(d['answer'])\n",
        "    print('------------------')\n",
        "    print()\n",
        "    i += 1\n",
        "  return predictions\n",
        "\n",
        "def run_batches(pipe, all_messages, generation_args, dataset_split):\n",
        "  '''\n",
        "  Runs the model pipeline on input data in a batched manner. No printing.\n",
        "  We don't really use this function, but it may be useful for seeing one way of\n",
        "  running multiple input examples through a pipeline in one call.\n",
        "\n",
        "  @inputs:\n",
        "  - pipe: a transformer pipeline\n",
        "  - all_messages: a list of chat-formatted \"messages\"\n",
        "  - generation_args: parameters for running pipe\n",
        "  '''\n",
        "  predictions = []\n",
        "  message_dataset = Dataset.from_dict({\"chat\": all_messages})\n",
        "  i = 0\n",
        "  for out in pipe(KeyDataset(message_dataset, \"chat\"), **generation_args):\n",
        "    response = out[0]['generated_text'].strip()\n",
        "    prompt = all_messages[i][1]['content'].strip()\n",
        "\n",
        "    d = {}\n",
        "    example = dataset[dataset_split][i]\n",
        "    d['question'] = example['question']\n",
        "    d['answer'] = example['answer']\n",
        "    d['prediction'] = response\n",
        "    d['prompt'] = prompt\n",
        "    predictions.append(d)\n",
        "    i += 1\n",
        "  return predictions\n",
        "\n",
        "def run_model_on_one_prompt(model, tokenizer, dataset_split, prompt_template, n_examples=-1, do_batches=True):\n",
        "  '''\n",
        "  @inputs:\n",
        "  - model: A Hugging Face transformers model, e.g. one loaded with \"AutoModelForCausalLM.from_pretrained\"\n",
        "  - tokenizer: A Hugging Face transformers tokenizer\n",
        "  - dataset_split: a string representing the split of the dataset to run the model on\n",
        "  - prompt_template: one prompt template\n",
        "  - n_examples: an integer noting the number of examples to run the model. If -1, run the model on all examples\n",
        "  - do_batches: a boolean about whether to run and print task examples one by one, or run them in a batched manner\n",
        "\n",
        "  @outputs:\n",
        "  - predictions: a list of dictionaries, each containing one math question, ground truth (if given), and model predictions\n",
        "  '''\n",
        "  torch.random.manual_seed(0)\n",
        "\n",
        "  pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "  )\n",
        "\n",
        "  generation_args = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "  }\n",
        "\n",
        "  all_messages = format_chat_prompt(dataset_split, prompt_template, n_examples=n_examples)\n",
        "\n",
        "  if do_batches:\n",
        "    predictions = run_batches(pipe, all_messages, generation_args, dataset_split)\n",
        "  else:\n",
        "    predictions = run_one_by_one(pipe, all_messages, generation_args, dataset_split)\n",
        "\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "UkE_v04PAN0W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following cell runs our language model on five `math` examples and prints inputs and outputs. It should take around 1+ minute to run."
      ],
      "metadata": {
        "id": "7q_cxQjmiMMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = run_model_on_one_prompt(model, tokenizer, 'math', zero_shot_prompt, n_examples=5, do_batches=False)"
      ],
      "metadata": {
        "id": "AftN58aithSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 Think to yourself: what do you notice about the inputs and outputs from the model? How would you evaluate or compare model responses against ground truth answers?\n",
        "\n",
        "The funny looking tokens, e.g. `<|system|>`, are called \"special tokens\". Special tokens are introduced into language models' vocabularies for purposes such as delineating the beginning/end of a prompt or adding chat structure (learn more [here](https://huggingface.co/learn/agents-course/en/unit1/messages-and-special-tokens))."
      ],
      "metadata": {
        "id": "FB1vHKbdCXrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# view what `predictions` looks like\n",
        "print(json.dumps(predictions[0], indent=4))"
      ],
      "metadata": {
        "id": "T77xe8jhPZ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Few-shot Prompting (20 points)\n",
        "\n",
        "A common way to improve models' performance is by including demonstrative examples in prompts. This paradigm is called *in-context learning*.\n",
        "\n",
        "Providing examples can also encourage consistent output formatting, which helps with extracting models' answers for evaluation.\n",
        "\n",
        "Run the following cell to load in some exemplars:"
      ],
      "metadata": {
        "id": "5WqjQP4iNRNJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "default_exemplars = []\n",
        "with open('exemplars.jsonl', 'r') as infile:\n",
        "  for line in infile:\n",
        "    default_exemplars.append(json.loads(line))\n",
        "print(json.dumps(default_exemplars[0], indent=4))\n",
        "print(\"Total number of exemplars:\", len(default_exemplars))"
      ],
      "metadata": {
        "id": "YIH-5CNvbfLn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here's what a two-shot prompt template would look like, structurally:\n",
        "\n",
        "```\n",
        "Your task is to solve math questions. Keep your response brief, and conclude your response with a numeric answer.\n",
        "\n",
        "Question: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
        "Answer: 6\n",
        "\n",
        "Question: Leah had 32 chocolates and her sister had 42. If they ate 35, how many pieces do they have left in total?\n",
        "Answer: 39\n",
        "\n",
        "Question: {question}\n",
        "Answer:\n",
        "```\n",
        "\n",
        "Based on what we observe when running our model with a zero-shot prompt, we also add an instruction encouraging the model to *keep its response brief*. Prompt engineering is an iterative process, where we often adjust instructions based on observations made from small-scale experiments. In addition, a general pitfall common to machine learning is [underspecification](https://arxiv.org/abs/2011.03395). In a world of prompting generative AI models, this pitfall now includes now includes cases where instructions are ambiguous or do not specify certain information (read this [paper](https://arxiv.org/abs/2210.05815) to learn more).\n",
        "\n",
        "Your task:\n",
        "- (20 points) Complete the following function, which creates a prompt template that contains $n$ shots. Use the above two-shot example as a guide for how your $n$-shot prompt should look like."
      ],
      "metadata": {
        "id": "IGtRkMOHhC7e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_few_shot_prompt(exemplars, n=1):\n",
        "  '''\n",
        "  Creates a few shot prompt\n",
        "  @inputs\n",
        "  - exemplars: a list of dictionaries of the format {'question': '', 'target': ''}, where each\n",
        "  dictionary is a demonstrative example of the task\n",
        "  - n: the number of exemplars to sample\n",
        "  '''\n",
        "  assert n <= len(exemplars)\n",
        "  random.seed(0)\n",
        "\n",
        "  prompt_prefix = 'Your task is to solve math questions. Keep your response brief, and conclude your response with a numeric answer.\\n\\n'\n",
        "  shot_template = \"Question: {exemplar_question}\\nAnswer: {exemplar_answer}\\n\\n\"\n",
        "  prompt_suffix = \"Question: {question}\\nAnswer:\"\n",
        "\n",
        "  prompt = prompt_prefix\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "  ### END SOLUTION\n",
        "\n",
        "  return prompt"
      ],
      "metadata": {
        "id": "S6Jgy9TZNTlt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at whether and how your function works:"
      ],
      "metadata": {
        "id": "cJJuBhRFTcpg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "three_shot_prompt = create_few_shot_prompt(default_exemplars, n=3)\n",
        "print(three_shot_prompt)\n",
        "print('---------------------')\n",
        "five_shot_prompt = create_few_shot_prompt(default_exemplars, n=5)\n",
        "print(five_shot_prompt)"
      ],
      "metadata": {
        "id": "e79vahHx_Bqh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = run_model_on_one_prompt(model, tokenizer, 'math', five_shot_prompt, n_examples=5, do_batches=False)"
      ],
      "metadata": {
        "id": "BS1ArAGTTYDb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 Think to yourself: what do you notice about the inputs and outputs from this prompt? How do they differ from our minimal zero-shot approach? What problems remain?"
      ],
      "metadata": {
        "id": "vHRJpJlwi915"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Chain-of-thought Prompting (10 points)\n",
        "\n",
        "In the model responses in previous sections, you may have observed that the model's lengthy response includes its intermediate problem solving steps. This behavior arises because models are now developed to vocalize their \"[chain of thought](https://arxiv.org/abs/2201.11903)\" before settling on an answer (or \"[think step by step](https://arxiv.org/abs/2205.11916)\"). In earlier language models, one had to instruct models to do this \"thinking\" or demonstrate it to elicit it, but more recent models are built to do it by default, especially for math problems.\n",
        "\n",
        "Though current models tend to be expressive with step-by-step explanations by default, it's still useful to explicitly show models how to \"think\". That is, few-shot exemplars *that include CoT* can specify how and where models should do their \"thinking\".\n",
        "\n",
        "Your task:\n",
        "- (10 points) Look through the data files we loaded at the beginning of this notebook, and identify the one that contains CoT exemplars. Then, in the cell below, populate a list of CoT exemplars from that data file, similar to `default_exemplars`. The printed output of the cell below should be one json."
      ],
      "metadata": {
        "id": "JbUMuoaMO6dx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cot_exemplars = []\n",
        "### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "### END SOLUTION\n",
        "print(json.dumps(cot_exemplars[0], indent=4))"
      ],
      "metadata": {
        "id": "qaonmHzrcxSN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "three_shot_cot_prompt = create_few_shot_prompt(cot_exemplars, n=3)\n",
        "print(three_shot_cot_prompt)\n",
        "print('---------------------')\n",
        "five_shot_cot_prompt = create_few_shot_prompt(cot_exemplars, n=5)\n",
        "print(five_shot_cot_prompt)"
      ],
      "metadata": {
        "id": "0KGLL-8IO809"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = run_model_on_one_prompt(model, tokenizer, 'math', three_shot_cot_prompt, n_examples=5, do_batches=False)"
      ],
      "metadata": {
        "id": "4KSW61oCkIiB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 Think to yourself: what do you notice about the inputs and outputs from this prompt? What is different from our few-shot approach before? How does it compare to our zero-shot approach? Some researchers have suggested that CoT is mainly useful for math and symbolic reasoning tasks ([source](https://arxiv.org/abs/2409.12183v2)). Why do you think this is?"
      ],
      "metadata": {
        "id": "SWqbbkIBqRmx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Self-consistency (25 points)\n",
        "\n",
        "In the examples we ran on in earlier sections, it seems like Phi 3.5 mini doesn't do too badly on math.\n",
        "\n",
        "To keep our model on its toes, we extracted a set of challenging math problems from `math.jsonl` and put them in `challenge.jsonl`. These examples are in the `challenge` split of the `dataset` we loaded at the beginning of our notebook.\n",
        "\n",
        "Let's run our previous CoT approach on some examples from this challenge set and see how well it does."
      ],
      "metadata": {
        "id": "MWDDAn9aQyf3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "predictions = run_model_on_one_prompt(model, tokenizer, 'challenge', three_shot_cot_prompt, n_examples=-1, do_batches=False)"
      ],
      "metadata": {
        "id": "ohl0730_4Bw1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It seems like we'll need to cook up some more strategies to see if we can do better.\n",
        "\n",
        "One approach is to sample from multiple output trajectories and aggregate their answers with majority voting. This strategy is called [self-consistency](https://arxiv.org/abs/2203.11171), and has been shown to improve CoT for some tasks. Self-consistency is a form of *ensembling*. The authors of the paper call it \"self-ensembling\", because instead of aggregating outputs across several different models, we're aggregating outputs from the same model.\n",
        "\n",
        "Your task:\n",
        "- (10 points) In `run_model_with_self_consistency`, modify the `generation_args` dictionary by adding *four* parameters. These parameters should set the model temperature to be 0.7, turn on sampling decoding, set top-$k$ to be 40, and request the pipeline to return 5 sequences per prompt. Check Hugging Face `transformers` documentation to see how one should set these parameters ([generate_kwargs](https://huggingface.co/docs/transformers/main_classes/text_generation) of [pipeline](https://huggingface.co/docs/transformers/v4.49.0/en/main_classes/pipelines#transformers.pipeline))."
      ],
      "metadata": {
        "id": "1o_eR0QD4HJj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_model_with_self_consistency(model, tokenizer, dataset_split, prompt_template, n_examples=-1, do_batches=True):\n",
        "  '''\n",
        "  @inputs:\n",
        "  - model: A Hugging Face transformers model, e.g. one loaded with \"AutoModelForCausalLM.from_pretrained\"\n",
        "  - tokenizer: A Hugging Face transformers tokenizer\n",
        "  - dataset_split: a string representing the split of the dataset to run the model on\n",
        "  - prompt_template: one prompt template\n",
        "  - n_examples: an integer noting the number of examples to run the model. If -1, run the model on all examples\n",
        "  - do_batches: a boolean about whether to run and print task examples one by one, or run them in a batched manner\n",
        "\n",
        "  @outputs:\n",
        "  - predictions: a list of dictionaries, each containing one math question, ground truth (if given), and model predictions\n",
        "  '''\n",
        "  torch.random.manual_seed(0)\n",
        "\n",
        "  pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "  )\n",
        "\n",
        "  generation_args = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"return_full_text\": False,\n",
        "    ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "    ### END SOLUTION\n",
        "  }\n",
        "\n",
        "  all_messages = format_chat_prompt(dataset_split, prompt_template, n_examples=n_examples)\n",
        "\n",
        "  predictions = run_self_consistency(pipe, all_messages, generation_args, dataset_split)\n",
        "\n",
        "  return predictions\n",
        "\n",
        "def run_self_consistency(pipe, all_messages, generation_args, dataset_split):\n",
        "  predictions = []\n",
        "  i = 0\n",
        "  for messages in tqdm(all_messages):\n",
        "    output = pipe(messages, **generation_args)\n",
        "    prompt = all_messages[i][1]['content'].strip()\n",
        "    responses = []\n",
        "    # gather each response - there is now more than one per prompt!\n",
        "    for o in output:\n",
        "      responses.append(o['generated_text'].strip())\n",
        "\n",
        "    # populate the output dictionary\n",
        "    d = {}\n",
        "    example = dataset[dataset_split][i]\n",
        "    d['question'] = example['question']\n",
        "    d['answer'] = example['answer']\n",
        "    d['prediction'] = responses\n",
        "    d['prompt'] = prompt\n",
        "    predictions.append(d)\n",
        "\n",
        "    i += 1\n",
        "  return predictions"
      ],
      "metadata": {
        "id": "U08xeIyoQz0-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The following should take around ~2 minutes to run."
      ],
      "metadata": {
        "id": "S79AckjRXpaT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "self_consistency_predictions = run_model_with_self_consistency(model, tokenizer, 'challenge', three_shot_cot_prompt, n_examples=-1, do_batches=False)"
      ],
      "metadata": {
        "id": "rChDY9_LHkOo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your tasks:\n",
        "- (10 points) Implement the `extract_answers_from_responses` function.\n",
        "- (5 points) Implement the `get_majority_vote` function."
      ],
      "metadata": {
        "id": "EVQamCqUP0GE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_answers_from_responses(response_list):\n",
        "  '''\n",
        "  @input\n",
        "  - response_list: a list of model responses, where each response is a string\n",
        "\n",
        "  @output\n",
        "  - a list of answers, where each answer is a string representing a numeric value\n",
        "\n",
        "  You can decide how you return ill-formed responses, such as responses that do not\n",
        "  end with \"The answer is ____.\"\n",
        "\n",
        "  Your function should handle some punctuation marks in numbers such as '.', '$', and ',', e.g.\n",
        "    9.00 -> 9\n",
        "    $9 -> 9\n",
        "    9,000 -> 9000\n",
        "  '''\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "  ### END SOLUTION"
      ],
      "metadata": {
        "id": "RPH93_i3N2Vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_majority_vote(answer_list):\n",
        "  '''\n",
        "  Get the most common answer from a list of answers.\n",
        "  Ties may be broken arbitrarily.\n",
        "\n",
        "  @input\n",
        "  - response_list: a list of answers, where each answer is a string representing a numeric value.\n",
        "  For example, one response list might look like ['10', '10', '21', '10', '6']\n",
        "\n",
        "  @output\n",
        "  - the most common answer, as a string\n",
        "  '''\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "  ### END SOLUTION"
      ],
      "metadata": {
        "id": "kjehe4mLOlQ3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Run the following to test out your functions and double check they work:"
      ],
      "metadata": {
        "id": "H3bloxjqctB1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "test1 = extract_answers_from_responses(['We add $10,000 to $60,000 to make $70,000. The answer is $70,000.'])\n",
        "assert test1 == ['70000']\n",
        "test2 = extract_answers_from_responses(['Mary has 4 eggs and Steve gives her 2, which totals 6 eggs. The answer is 6.'])\n",
        "assert test2 == ['6']\n",
        "test3 = get_majority_vote(['10', '10', '21', '10', '6'])\n",
        "assert test3 == '10'"
      ],
      "metadata": {
        "id": "WQNLiZr8cwmo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for pred in self_consistency_predictions:\n",
        "  extracted_answers = extract_answers_from_responses(pred['prediction'])\n",
        "  majority_vote = get_majority_vote(extracted_answers)\n",
        "\n",
        "  print(\"---- QUESTION ----\")\n",
        "  print(pred['question'])\n",
        "  print(\"---- MODEL RESPONSES ----\")\n",
        "  print(extracted_answers)\n",
        "  print(\"---- MAJORITY RESPONSE ----\")\n",
        "  print(majority_vote)\n",
        "  print(\"---- GROUND TRUTH ----\")\n",
        "  print(pred['answer'])\n",
        "  print('------------------')\n",
        "  print()"
      ],
      "metadata": {
        "id": "tTrqaN1cNqGg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 Think to yourself: How much does self-consistency help with tackling problems in our challenge set?\n",
        "\n",
        "Arithmetic is a common domain for evaluating language models because each example has only one true answer, and its answers are easily verifiable. What might self-consistency look like for more open-ended tasks? Check out this [paper](https://arxiv.org/abs/2311.17311) for one possibility."
      ],
      "metadata": {
        "id": "YXWkNO-75Ado"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# From Prompts to Conversations (20 points)\n",
        "\n",
        "When you use an online chatbot, such as ChatGPT (https://chat.openai.com/) or Claude (https://claude.ai/), you may notice that each chat can be a series of messages back and forth, rather than just one user prompt and model response like we have seen so far.\n",
        "\n",
        "In practice, these chat chains involve re-prompting a model with successively longer prompts, where later exchanges include the \"conversation history\" of earlier exchanges.\n",
        "\n",
        "We'll pretend that we have a user who has a multi-part math problem they'd like to solve, and they're chatting with Phi 3.5 to solve each part. To create this \"chat\", we'll prepend the previous subquestions and their answers to create a series of messages.\n",
        "\n",
        "For example, this is what is inputted into a model each time the user asks a follow-up question in a conversation:\n",
        "\n",
        "Prompt 1\n",
        "\n",
        "```\n",
        "User: Cappuccinos cost $2, iced teas cost $3, cafe lattes cost $1.5 and espressos cost $1 each. Sandy orders some drinks for herself and some friends. She orders three cappuccinos, two iced teas, two cafe lattes, and two espressos. How much did the cappuccinos cost?\n",
        "```\n",
        "\n",
        "Prompt 2\n",
        "\n",
        "```\n",
        "User: Cappuccinos cost $2, iced teas cost $3, cafe lattes cost $1.5 and espressos cost $1 each. Sandy orders some drinks for herself and some friends. She orders three cappuccinos, two iced teas, two cafe lattes, and two espressos. How much did the cappuccinos cost?\n",
        "\n",
        "Model: Each cappucino costs $2, and Sandy ordered three of them. $2 x 3 cappucinos is $6.\n",
        "\n",
        "User: How much did the iced teas and cafe lattes cost together?\n",
        "```\n",
        "\n",
        "Prompt 3\n",
        "\n",
        "```\n",
        "User: Cappuccinos cost $2, iced teas cost $3, cafe lattes cost $1.5 and espressos cost $1 each. Sandy orders some drinks for herself and some friends. She orders three cappuccinos, two iced teas, two cafe lattes, and two espressos. How much did the cappuccinos cost?\n",
        "\n",
        "Model: Each cappucino costs $2, and Sandy ordered three of them. $2 x 3 cappucinos is $6.\n",
        "\n",
        "User: How much did the iced teas and cafe lattes cost together?\n",
        "\n",
        "Model: The cappucinos altogether cost $6. Each cafe latte is $1.50 and Sandy ordered two of them. $6.00 + $1.50 x 2 = $6.00 + $3.00 = $9.00.\n",
        "\n",
        "User: How much did all of the drinks cost in total?\n",
        "```\n",
        "\n",
        "and so on.\n"
      ],
      "metadata": {
        "id": "V_bIk91fQ0yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "question_chains = [\n",
        "    {\n",
        "    'premise': 'Four years ago, Kody was only half as old as Mohamed. Mohamed is currently twice 30 years old.',\n",
        "    'questions': [\"How old is Mohamed right now?\", \"How old was he four years ago?\", \"How old was Kody four years ago?\", \"How old is Kody now?\"]\n",
        "    },\n",
        "    {\n",
        "    'premise': 'Sam, Sid, and Steve brought popsicle sticks for their group activity in their Art class. Sam has thrice as many as Sid, and Sid has twice as many as Steve. Steve has 12 popsicle sticks.',\n",
        "    'questions': [\"How many popsicle sticks does Sid have?\", \"How many popsicle sticks does Sam have?\", \"How many popsicle sticks can these three people use for their Art class activity?\"]\n",
        "    }\n",
        "]"
      ],
      "metadata": {
        "id": "3kbo8C9SfY7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Your task:\n",
        "- (20 points) Complete the following function to complete the conversation chain between the user and Phi 3.5 mini. Your solution should call `pipe` with `generation_args`."
      ],
      "metadata": {
        "id": "NoI0MvVSzNwk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_conversation(model, tokenizer, question_chains):\n",
        "  '''\n",
        "  @inputs:\n",
        "  - model: A Hugging Face transformers model, e.g. one loaded with \"AutoModelForCausalLM.from_pretrained\"\n",
        "  - tokenizer: A Hugging Face transformers tokenizer\n",
        "  - dataset_split: a string representing the split of the dataset to run the model on\n",
        "  - prompt_template: one prompt template\n",
        "  - n_examples: an integer noting the number of examples to run the model. If -1, run the model on all examples\n",
        "  - do_batches: a boolean about whether to run and print task examples one by one, or run them in a batched manner\n",
        "\n",
        "  @outputs:\n",
        "  - predictions: a list of dictionaries, each containing one math question, ground truth (if given), and model predictions\n",
        "  '''\n",
        "  torch.random.manual_seed(0)\n",
        "\n",
        "  pipe = pipeline(\n",
        "        \"text-generation\",\n",
        "        model=model,\n",
        "        tokenizer=tokenizer,\n",
        "  )\n",
        "\n",
        "  generation_args = {\n",
        "    \"max_new_tokens\": 256,\n",
        "    \"return_full_text\": False,\n",
        "    \"do_sample\": False,\n",
        "  }\n",
        "\n",
        "  all_rounds = [] # a list of list of inputs\n",
        "  for chain in question_chains:\n",
        "    premise = chain['premise']\n",
        "    questions = chain['questions']\n",
        "    messages = [\n",
        "        {\"role\": \"system\", \"content\": \"You are a helpful assistant; follow the instructions in the prompt.\"}, # system prompt\n",
        "    ]\n",
        "    rounds = []\n",
        "    for i, q in enumerate(questions):\n",
        "      # this part simulates the user, who keeps asking followup questions\n",
        "      if i == 0:\n",
        "        # the first input includes the math questions' premise\n",
        "        prompt = premise + ' ' + q\n",
        "      else:\n",
        "        prompt = q\n",
        "      messages.append({\"role\": \"user\", \"content\": prompt})\n",
        "\n",
        "      # YOUR TASK is to continue the model's side of the conversation chain\n",
        "      ### BEGIN SOLUTION\n",
        "\n",
        "\n",
        "\n",
        "      ### END SOLUTION\n",
        "\n",
        "      rounds.append(tokenizer.apply_chat_template(messages, tokenize=False))\n",
        "    all_rounds.append(rounds)\n",
        "\n",
        "  return all_rounds"
      ],
      "metadata": {
        "id": "THWMLCvVBuMM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "all_rounds = run_conversation(model, tokenizer, question_chains)"
      ],
      "metadata": {
        "id": "J_adu1eWxh6K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's take a look at how one conversation grew:"
      ],
      "metadata": {
        "id": "W6ueGYj_1zCk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"***** First conversational exchange: *****\")\n",
        "print(all_rounds[0][0])\n",
        "print()\n",
        "print('-----------------------------------------------')\n",
        "print()\n",
        "\n",
        "print(\"***** Entire conversational exchange: *****\")\n",
        "print(all_rounds[0][-1])\n",
        "print()"
      ],
      "metadata": {
        "id": "Hd_NruV80sNs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Though special tokens are useful for models, they may not make text legible to us. Here's another view of this mathy conversation:"
      ],
      "metadata": {
        "id": "5Xw9eQ2m3SLR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def pretty_print_round(round):\n",
        "  round = round.replace('<|end|>', '').replace('<|endoftext|>', '')\n",
        "  round = round.replace(\"<|system|>\\n\", '⚙️: ')\n",
        "  round = round.replace(\"<|user|>\\n\", '\\n🧑:\\n')\n",
        "  round = round.replace(\"<|assistant|>\\n\", '\\n🤖:\\n')\n",
        "  return round\n",
        "\n",
        "print(\"***** First conversational exchange: *****\")\n",
        "print(pretty_print_round(all_rounds[0][0]))\n",
        "print()\n",
        "print('-----------------------------------------------')\n",
        "print()\n",
        "\n",
        "print(\"***** Entire conversational exchange: *****\")\n",
        "print(pretty_print_round(all_rounds[0][-1]))\n",
        "print()"
      ],
      "metadata": {
        "id": "v5Ja8MLb1Sbn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing and Submission\n",
        "\n",
        "Congratulations on finishing HW3! Please ensure that you submit this completed notebook onto Gradescope. Make sure all cells in the notebook are run so that print statements are visible. The notebook you upload to Gradescope must be named **HW3.ipynb**.\n",
        "\n",
        "`File` --> `Download` --> `Download .ipynb`"
      ],
      "metadata": {
        "id": "0Vl-IpRb5Uej"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PMpMxLzC2zkv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}