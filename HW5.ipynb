{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ethanherrera/info159/blob/main/HW5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Homework 5: Relation Extraction\n",
        "\n",
        "### Due Date: April 10 (11:59pm)\n",
        "\n",
        "**Total Points: 100 points**\n",
        "\n",
        "**Make a copy** of this notebook and work only on your copy, so that your work is retained. Make sure your copy is named `HW5.ipynb`.  \n",
        "\n",
        "# Introduction\n",
        "\n",
        "In this assignment, we'll examine how one could use **bootstrapping** to extract relations between entities from a dataset.\n",
        "\n",
        "Imagine you have been appointed to manage a large and ecologically diverse preserve. Here, the wifi is shaky, and the cell service is non-existent. It'll be a nice respite from trudging through endless midterms at Berkeley.\n",
        "\n",
        "However, this morning, a group of long-tailed macques ambushed you and stole your paper records detailing a taxonomy of the animals. You're left in a troubling situation, as without this taxonomy, it's tough to know which animals are which types and properly care for them. What are giraffes? Cephalopods? You're not too sure.\n",
        "\n",
        "The zoologists are on vacation, and have only left behind a collection of long-form notes describing the animals. Your only hope is to use relation extraction methods from INFO 159/259 to restructure these notes and get a head start on rebuilding the stolen taxonomy.\n",
        "\n",
        "Your knowledge of animals is pretty flimsy. Luckily, you recall from Jurafsky & Martin Chapter 20 the following algorithm for bootstrapping:\n",
        "\n",
        "```\n",
        "function BOOTSTRAP(Relation_R) returns new_relation_tuples\n",
        "  tuples ← Gather a set of seed tuples that have relation R\n",
        "  iterate\n",
        "    sentences ← find sentences that contain entities in tuples\n",
        "    patterns ← generalize the context between and around entities in sentences\n",
        "    new_pairs ← use patterns to identify more tuples\n",
        "    new_pairs ← new_pairs with high confidence\n",
        "    tuples ← tuples + new_pairs\n",
        "  return tuples\n",
        "```\n",
        "\n",
        "In this homework, we'll demonstrate the affordances and caveats of this algorithm by walking through one iteration. We'll gather phrases that are suggestive of *is-a* relations between animals and their types, and recover new hyponym-hypernym pairs to save the day.  \n",
        "\n",
        "### Remember:\n",
        "\n",
        "**Only** modify text or code in between `### BEGIN SOLUTION` and `### END SOLUTION` lines.\n"
      ],
      "metadata": {
        "id": "Ey-Myjq2P5XH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install dafsa"
      ],
      "metadata": {
        "id": "fYQE4gJoVUzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import itertools\n",
        "from collections import defaultdict, Counter\n",
        "from tqdm import tqdm\n",
        "import networkx as nx\n",
        "from dafsa import DAFSA\n",
        "import re\n",
        "import random"
      ],
      "metadata": {
        "id": "FwOzMlnKzTZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset prep\n",
        "\n",
        "You think it's a bit wild (pun intended) that the records at this animal preserve are still all on paper. You digitize the zoologists' handwritten notes, and preprocess your dataset to include token indices of nouns that refer to animals in your nature preserve (ANIMAL), and token indices of other nouns detected by a spaCy dependency parser (NOUN).\n",
        "\n",
        "Download this digitized file of pre-tagged notes by running the following cell."
      ],
      "metadata": {
        "id": "6bp9YwZiU_FL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/lucy3/temp/main/tagged_sentences.txt"
      ],
      "metadata": {
        "id": "XD55b4bz4ych"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this tab-delimited file, the first column is the lemmatized name of an animal, the second column is a sentence, and the third column is a tokenized, lemmatized form of that sentence. The remaining columns are indices of animals and non-target-animal nouns in the sentence."
      ],
      "metadata": {
        "id": "v3Xvjfr1asYf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with open('tagged_sentences.txt', 'r') as infile:\n",
        "  for line in infile:\n",
        "    print(line.strip().split('\\t')) # view each column\n",
        "    break"
      ],
      "metadata": {
        "id": "aHX1UWKFZg0U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here are all animal *is-a* relations that you can remember right now. This is your set of seed tuples for bootstrapping."
      ],
      "metadata": {
        "id": "THEBoBBUVDKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_tuples = [('kangaroo', 'marsupial'), ('kangaroo', 'mammal'), ('horse', 'mammal'), ('toucan', 'bird'),\n",
        "                ('crocodile', 'reptile'), ('ant', 'anthropod'), ('dinosaur', 'reptile'), ('hawk', 'bird'),\n",
        "                ('eagle', 'bird'), ('lark', 'bird'), ('toad', 'amphibian'), ('frog', 'amphibian'), ('ibis', 'bird'),\n",
        "                ('oyster', 'shellfish'), ('pigeon', 'bird'), ('shark', 'fish'), ('hummingbird', 'bird'),\n",
        "                ('jellyfish', 'cnidarian'), ('octopus', 'cephalopod'), ('anteater', 'xenarthan'), ('moose', 'mammal'),\n",
        "                ('dragonfly', 'insect'), ('camel', 'mammal'), ('beetle', 'insect'), ('kinkajou', 'mammal'), ('alpaca', 'mammal')]"
      ],
      "metadata": {
        "id": "RTwjy9xaVwpC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Mining for patterns\n",
        "\n",
        "Let's find sentences in the zoologists' notes that contain our seed set of `(ANIMAL, NOUN)` tuples, and extract the context around them.\n",
        "\n",
        "Each phrasal pattern has three key parts:\n",
        "- the middle context between one 'ANIMAL' and one 'NOUN'\n",
        "- a left context of at most $n$ space-separated tokens to the left\n",
        "- a right context of at most $m$ space-separated tokens to the right.\n",
        "\n",
        "For example, here are some phrasal patterns with $n=2$ and $m=2$:\n",
        "\n",
        "- `the ANIMAL be a NOUN .` ← \"The giraffe is a mammal.\",\n",
        "- `as a NOUN , a ANIMAL have fur` ← \"As a mammal, a giraffe has fur.\".\n",
        "\n",
        "### Deliverable 1 (40 pts ✨)\n",
        "\n",
        "Implement the following function that takes in token indices of an `(ANIMAL, NOUN)` pair and a preprocessed sentence from your zoologists' notes. The output is a phrasal pattern.\n",
        "\n",
        "For example, given the preprocessed (lemmatized & tokenized) sentence\n",
        "\n",
        "```\n",
        "['tall', 'and', 'long', '-', 'neck', ',', 'the', 'giraffe', 'be', 'a', 'mammal', ',', 'and', 'it', 'eat', 'leaf', 'from', 'high', 'branch', 'of', 'tree', '.']\n",
        "```\n",
        "\n",
        "and indices of `('giraffe', 'mammal')` as `(7, 10)`, and $n=2$ and $m=2$, your function should output\n",
        "\n",
        "```\n",
        ", the ANIMAL be a NOUN , and\n",
        "```\n",
        "\n",
        "With $n=5$ and $m=5$, your function should instead output\n",
        "\n",
        "```\n",
        "long - neck , the ANIMAL be a NOUN , and it eat leaf\n",
        "```"
      ],
      "metadata": {
        "id": "iEd61M0k1s73"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_text_into_pattern(animal_noun, tokenized_sentence, n, m):\n",
        "  '''\n",
        "  @inputs:\n",
        "  - animal_noun: a tuple of (ANIMAL, NOUN) indices\n",
        "    For example, in the sentence \"The horse is a domesticated, one-toed, hoofed mammal.\",\n",
        "    animal_noun == (1, 11),\n",
        "    where token index 1 is \"horse\" and token index 11 is \"mammal\".\n",
        "  - tokenized_sentence: a lemmatized, tokenized sentence, represented as a list of tokens\n",
        "  - n: an integer indicating the size, or number of tokens, of the left context window\n",
        "  - m: an integer indicating the size, or number of tokens, of the right content window\n",
        "\n",
        "  @outputs:\n",
        "  - phrase: a string containing left, middle, and right context around ANIMAL and NOUN\n",
        "  '''\n",
        "  phrase = ''\n",
        "\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "  ### END SOLUTION\n",
        "\n",
        "  return phrase.strip()"
      ],
      "metadata": {
        "id": "JhNgUoKI2Szw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Check that your function works as expected by running the following cell."
      ],
      "metadata": {
        "id": "ZHfdPkEHhr5O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phrase = transform_text_into_pattern((1, 4),\n",
        "      ['the', 'giraffe', 'be', 'a', 'mammal', '.'], 5, 5)\n",
        "assert(phrase == 'the ANIMAL be a NOUN .')\n",
        "\n",
        "phrase = transform_text_into_pattern((7, 10),\n",
        "      ['tall', 'and', 'long', '-', 'neck', ',', 'the', 'giraffe', 'be', 'a',\n",
        "       'mammal', ',', 'and', 'it', 'eat', 'leaf', 'from', 'high',\n",
        "       'branch', 'of', 'tree', '.'], 5, 5)\n",
        "assert(phrase == 'long - neck , the ANIMAL be a NOUN , and it eat leaf')\n",
        "\n",
        "phrase = transform_text_into_pattern((7, 10),\n",
        "      ['tall', 'and', 'long', '-', 'neck', ',', 'the', 'giraffe', 'be', 'a',\n",
        "       'mammal', ',', 'and', 'it', 'eat', 'leaf', 'from', 'high',\n",
        "       'branch', 'of', 'tree', '.'], 2, 2)\n",
        "assert(phrase == ', the ANIMAL be a NOUN , and')\n",
        "\n",
        "phrase = transform_text_into_pattern((5, 2),\n",
        "      ['as', 'a', 'mammal', ',', 'a', 'giraffe', 'have', 'fur'], 5, 5)\n",
        "assert(phrase == 'as a NOUN , a ANIMAL have fur')"
      ],
      "metadata": {
        "id": "Rk_Bys5Ch_NC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we call your function to extract phrasal patterns based on the seed set of tuples. To keep things tidy, we restrict the middle context to be $\\leq 10$ tokens long."
      ],
      "metadata": {
        "id": "UCs02L9MoiS0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def parse_input_line(line):\n",
        "  contents = line.strip().split('\\t')\n",
        "  animal = contents[0]\n",
        "  sentence = contents[1]\n",
        "  sentence_toks = contents[2].split(' ')\n",
        "\n",
        "  # get animal and noun indices\n",
        "  animal_idx = []\n",
        "  noun_idx = []\n",
        "  for i in range(3, len(contents)):\n",
        "    if contents[i].startswith('ANIMAL'):\n",
        "      animal_idx.append(int(contents[i].split()[-1]))\n",
        "    elif contents[i].startswith('NOUN'):\n",
        "      noun_idx.append(int(contents[i].split()[-1]))\n",
        "  return animal, sentence, sentence_toks, animal_idx, noun_idx\n",
        "\n",
        "def extract_phrasal_patterns(seed_tuples):\n",
        "  '''\n",
        "  @inputs:\n",
        "  - seed_tuples: a set of seed tuples of (ANIMAL, NOUN)\n",
        "\n",
        "  @outputs:\n",
        "  - phrasal_patterns: a dictionary of patterns to their original sentences\n",
        "  '''\n",
        "  phrasal_patterns = defaultdict(list)\n",
        "\n",
        "  with open('tagged_sentences.txt', 'r') as infile:\n",
        "    for line in tqdm(infile, total=19587):\n",
        "      animal, sentence, sentence_toks, animal_idx, noun_idx = parse_input_line(line)\n",
        "\n",
        "      # get phrasal contexts of animal and noun pairs\n",
        "      for pair in itertools.product(animal_idx, noun_idx):\n",
        "        if (sentence_toks[pair[0]], sentence_toks[pair[1]]) in seed_tuples:\n",
        "          if abs(pair[0] - pair[1]) <= 10:\n",
        "            # calls the function that you wrote\n",
        "            phrase = transform_text_into_pattern(pair, sentence_toks, 5, 5)\n",
        "            phrasal_patterns[phrase].append(sentence)\n",
        "\n",
        "  return phrasal_patterns"
      ],
      "metadata": {
        "id": "ALW4o6bWhtDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "phrasal_patterns = extract_phrasal_patterns(seed_tuples)"
      ],
      "metadata": {
        "id": "MKh-IOUKvgs_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pattern = 'the ANIMAL be a domesticate , one - toed , hoofed NOUN .'\n",
        "print(phrasal_patterns[pattern])\n",
        "# this cell should print out \"['The horse is a domesticated, one-toed, hoofed mammal.']\""
      ],
      "metadata": {
        "id": "roFyD68v1EeC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "It is good to *generalize* the context around target entities, to account for irrelevant contextual variations. There are many ways one could approach this. Here, we'll replace infrequent parts of contexts with a wildcard `*`.\n",
        "\n",
        "We implement pattern generalization for you. To do so, we construct a directed acyclic word graph by looping through all of the phrasal patterns our seed tuples found. Each edge of this graph represents one word (unlike [the example figure on Wikipedia](https://en.wikipedia.org/wiki/Deterministic_acyclic_finite_state_automaton), where each edge is a letter/character). Each path starting at the root of this graph is a phrasal pattern, and edge weights are incremented based on how often they are traversed across all patterns. If an edge appears less than 3 times, it is replaced with a `*`.  Our implementation is heavily inspired by Section 3.3 of [this paper](https://arxiv.org/abs/1804.02525).\n",
        "\n",
        "For example, the sentences \"_**Giraffes** are large African hoofed **mammals** who eats leaves._\" and \"_**Giraffes** are tall **mammals** living in grassy, open woodlands._\" could be both represented by the pattern `ANIMAL be * NOUN *`."
      ],
      "metadata": {
        "id": "Vt-tYnHnbG99"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_graph(phrasal_patterns):\n",
        "    \"\"\"\n",
        "    Builds a directed word graph from a set of phrasal patterns.\n",
        "    Each word is weighted by its count in the graph.\n",
        "\n",
        "    @input:\n",
        "\n",
        "    @output:\n",
        "    - a networkx graph\n",
        "    \"\"\"\n",
        "    words = []\n",
        "    for pattern in phrasal_patterns:\n",
        "        words.append(pattern.split(' '))\n",
        "    d = DAFSA(words) # creates directed acyclic word graphs\n",
        "\n",
        "    # convert d into a networkx graph that is directed and has parallel edges\n",
        "    graph = nx.MultiDiGraph()\n",
        "    for node_id, node in d.nodes.items():\n",
        "      graph.add_node(node_id)\n",
        "      graph.nodes[node_id][\"final\"] = node.final\n",
        "\n",
        "    for left in d.nodes.values():\n",
        "        for label, right in left.edges.items():\n",
        "            l_id = left.node_id\n",
        "            r_id = right.node.node_id\n",
        "            graph.add_edge(l_id, r_id, weight=right.weight, label=label)\n",
        "\n",
        "    return graph\n",
        "\n",
        "def clean_up_patterns(all_patterns):\n",
        "  clean_patterns = set()\n",
        "  for pattern in all_patterns:\n",
        "    pattern_string = ' '.join(pattern)\n",
        "    # reduce repeated * into a single *\n",
        "    pattern_string = re.sub(r'(\\*\\s?)+', '* ', pattern_string)\n",
        "    # constraint: some middle context must exist\n",
        "    if 'ANIMAL * NOUN' in pattern_string or 'NOUN * ANIMAL' in pattern_string:\n",
        "      continue\n",
        "    clean_patterns.add(pattern_string.strip())\n",
        "  clean_patterns = list(clean_patterns)\n",
        "  return clean_patterns\n",
        "\n",
        "def get_generalized_paths(graph):\n",
        "  '''\n",
        "  @inputs:\n",
        "  - a networkx graph\n",
        "\n",
        "  @outputs:\n",
        "  - generalized phrasal patterns, as strings\n",
        "  '''\n",
        "  final_nodes = []\n",
        "  for node in graph.nodes(data=True):\n",
        "    if node[1]['final']:\n",
        "      final_nodes.append(node[0])\n",
        "  weight_threshold = 3 # number of times a transition/edge must exist for us to not wildcard it\n",
        "  do_not_wildcard = ['ANIMAL', 'NOUN'] # tokens that shouldn't be replaced with a wildcard\n",
        "\n",
        "  clustered_patterns = []\n",
        "  for end in final_nodes:\n",
        "    paths = nx.all_simple_edge_paths(graph, 0, end)\n",
        "    for path in paths:\n",
        "      this_path = []\n",
        "      for edge in path:\n",
        "          src = edge[0]\n",
        "          dst = edge[1]\n",
        "          edge_data = graph.get_edge_data(edge[0], edge[1])[edge[2]]\n",
        "          if edge_data['label'] not in do_not_wildcard and \\\n",
        "              edge_data['weight'] < weight_threshold:\n",
        "            this_path.append('*')\n",
        "          else:\n",
        "            this_path.append(edge_data['label'])\n",
        "      clustered_patterns.append(this_path)\n",
        "\n",
        "  clustered_patterns = clean_up_patterns(clustered_patterns)\n",
        "  clustered_patterns = sorted(clustered_patterns)\n",
        "  return clustered_patterns"
      ],
      "metadata": {
        "id": "cMuaHdF0t7FC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "graph = build_graph(phrasal_patterns)\n",
        "clustered_patterns = get_generalized_paths(graph)"
      ],
      "metadata": {
        "id": "msghVnO7GWlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "clustered_patterns"
      ],
      "metadata": {
        "id": "uvOCJF4AGYZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "As a sanity check for your implementation, the above cell should output patterns containing n-grams such as \"like other NOUN\", \"like most NOUN\", \"ANIMAL be\", and \"unlike other NOUN\".\n",
        "\n",
        "You may notice that some of these patterns are subsets of others. The extent to which we rely on `*` to be a catch-all for the context around ANIMAL and potential NOUNs affects precision, as we'll soon see.  "
      ],
      "metadata": {
        "id": "D97_fyXF4utO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Identify more animal-type tuples\n",
        "\n",
        "Now, let's use these patterns to find more potential *is-a* relations for the animals in our preserve.\n",
        "\n",
        "### Deliverable 2 (30 pts ✨)\n",
        "\n",
        "Your task is to write a function that takes in a candidate phrase containing ANIMAL and NOUN, and checks whether it matches with a pattern we mined above. Hint: use the regex package `re`."
      ],
      "metadata": {
        "id": "MyCNOXBXt7SQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def is_match(cand_phrase, pattern):\n",
        "  '''\n",
        "  @input:\n",
        "  - cand_phrase: a candidate phrase containing ANIMAL and NOUN, e.g. \"like other NOUN , the ANIMAL eat hay .\"\n",
        "  - pattern: a generalized pattern, e.g. \"like other NOUN , * ANIMAL *\"\n",
        "\n",
        "  @output:\n",
        "  - a boolean of True or False\n",
        "  '''\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "  ### END SOLUTION"
      ],
      "metadata": {
        "id": "31K3cabOC7ud"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test out your implementation by running the following cell."
      ],
      "metadata": {
        "id": "OIz1kTw_EruO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "output = is_match(\"like other NOUN , the ANIMAL eat hay .\", \"like other NOUN , * ANIMAL *\")\n",
        "assert output == True\n",
        "\n",
        "output = is_match(\"like other NOUN , ANIMAL eat hay .\", \"like other NOUN , * ANIMAL *\")\n",
        "assert output == False\n",
        "\n",
        "output = is_match(\"like other NOUN , ANIMAL eat hay .\", \"like other NOUN , ANIMAL *\")\n",
        "assert output == True\n",
        "\n",
        "output = is_match(\"ANIMAL be a domesticate , one - toed , hoofed NOUN .\", \"ANIMAL be * NOUN *\")\n",
        "assert output == True"
      ],
      "metadata": {
        "id": "rBuyfH-tEt6d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_candidates():\n",
        "  '''\n",
        "  @outputs:\n",
        "  - candidates: a list of (phrase, animal, noun), e.g. ('the ANIMAL be a domesticate , one - toed , hoofed NOUN .', 'horse', 'mammal')\n",
        "  '''\n",
        "  candidates = []\n",
        "  with open('tagged_sentences.txt', 'r') as infile:\n",
        "    for line in tqdm(infile, total=19587):\n",
        "      animal, sentence, sentence_toks, animal_idx, noun_idx = parse_input_line(line)\n",
        "\n",
        "      # get phrasal contexts of animal and noun pairs\n",
        "      for pair in itertools.product(animal_idx, noun_idx):\n",
        "        if abs(pair[0] - pair[1]) <= 10:\n",
        "          # calls the function that you wrote\n",
        "          phrase = transform_text_into_pattern(pair, sentence_toks, 5, 5)\n",
        "          candidates.append((phrase, sentence_toks[pair[0]], sentence_toks[pair[1]]))\n",
        "  return candidates\n",
        "\n",
        "def filter_for_matches(candidates, clustered_patterns):\n",
        "  matches = defaultdict(list)\n",
        "  for cand in tqdm(candidates):\n",
        "    for pattern in clustered_patterns:\n",
        "      if is_match(cand[0], pattern):\n",
        "        matches[pattern].append(cand)\n",
        "  return matches"
      ],
      "metadata": {
        "id": "EzdepoeH_XJ9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "candidates = get_candidates()\n",
        "matches = filter_for_matches(candidates, clustered_patterns)"
      ],
      "metadata": {
        "id": "q7qae8I3AOqf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Deliverable 3 (30 pts ✨)\n",
        "\n",
        "You remember from Jurafsky & Martin Ch. 20, page 8 that\n",
        "> Bootstrapping systems also assign confidence values to new tuples to avoid semantic drift. In semantic drift, an erroneous pattern leads to the introduction of erroneous tuples, which, in turn, lead to the creation of problematic patterns and the meaning of the extracted relations ‘drifts’.\n",
        "\n",
        "> Confidence values for patterns are based on balancing two factors: the pattern’s performance with respect to the current set of tuples and the pattern’s productivity in terms of the number of matches it produces in the document collection.\n",
        "\n",
        "With the above in mind, write the following function, `hits_and_finds()`, which returns the following:\n",
        "- `num_hits`: the number of unique (ANIMAL, NOUN) tuples in our seed set (`seed_tuples`) that a pattern $p$ matches while looking in our dataset.\n",
        "- `num_finds`: the total number of unique (ANIMAL, NOUN) tuples that $p$ finds in our dataset."
      ],
      "metadata": {
        "id": "GyLkUupmv-fS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def hits_and_finds(matches, p, seed_tuples):\n",
        "  '''\n",
        "  @inputs:\n",
        "  - matches: a dictionary of the format {generalized pattern: [list of tuples (preprocessed sentence, animal, noun) ] }\n",
        "  - p: a generalized pattern\n",
        "  - seed_tuples: a list of seed tuples of the format (animal, noun)\n",
        "\n",
        "  @outputs:\n",
        "  - num_hits: an int, or the number of unique seed animal-type tuples found by p\n",
        "  - num_finds: an int, or the total number of unique animal-type tuples found by p\n",
        "  '''\n",
        "  assert p in matches\n",
        "\n",
        "  ### BEGIN SOLUTION\n",
        "\n",
        "  ### END SOLUTION\n",
        "\n",
        "  return num_hits, num_finds"
      ],
      "metadata": {
        "id": "iROka_ODyIsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sanity check the implementation of your function with the following toy inputs."
      ],
      "metadata": {
        "id": "l8ssJQCbT5C-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "matches_toy = {\n",
        "    'the ANIMAL be a * NOUN *' : [\n",
        "              ('the ANIMAL be a domesticate , one - toed , hoofed NOUN .',\n",
        "               'horse',\n",
        "               'mammal'),\n",
        "              ('the ANIMAL be a medium - sized NOUN species that be often slightly',\n",
        "               'mallard',\n",
        "               'waterfowl'),\n",
        "              ('the ANIMAL be a social NOUN , form pack of two',\n",
        "               'meerkat',\n",
        "               'mammal'),\n",
        "              ('the ANIMAL be a NOUN .',\n",
        "               'horse',\n",
        "               'mammal'),\n",
        "              ]\n",
        "}\n",
        "p_toy = 'the ANIMAL be a * NOUN *'\n",
        "seed_tuples_toy = [('horse', 'mammal'), ('mallard', 'waterfowl'), ('toucan', 'bird')]\n",
        "\n",
        "num_hits, num_finds = hits_and_finds(matches_toy, p_toy, seed_tuples_toy)\n",
        "assert num_hits == 2 # because we hit ('horse', 'mammal') and ('mallard', 'waterfowl')\n",
        "assert num_finds == 3 # because we find ('horse', 'mammal'), ('mallard', 'waterfowl'), and ('meerkat', 'mammal')"
      ],
      "metadata": {
        "id": "1s4jYi6WT4e8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's inspect phrasal patterns and the `(ANIMAL, NOUN)` tuples they surface. For each pattern, we'll give examples of both hits and *new* finds."
      ],
      "metadata": {
        "id": "yU8RUEaGVQm9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def show_results(matches, seed_tuples):\n",
        "  '''\n",
        "  Prints candidate animal-type tuples and the patterns\n",
        "  that identified them.\n",
        "  '''\n",
        "  for pattern in sorted(matches.keys()):\n",
        "    num_hits, num_finds = hits_and_finds(matches, pattern, seed_tuples)\n",
        "    hits = []\n",
        "    new_finds = []\n",
        "    for tup in matches[pattern]:\n",
        "      animal_type = (tup[1], tup[2])\n",
        "      if animal_type in seed_tuples:\n",
        "        hits.append(tup)\n",
        "      else:\n",
        "        new_finds.append(tup)\n",
        "    if len(new_finds) > 20:\n",
        "      new_finds = random.sample(new_finds, 20)\n",
        "\n",
        "    random.seed(0)\n",
        "    print(\"Pattern:\", pattern)\n",
        "    print(\"# of hits: \", num_hits)\n",
        "    print(\"Hits:\")\n",
        "    for tup in hits:\n",
        "      print('\\t', tup[1], 'is a', tup[2], '←', tup[0])\n",
        "    print(\"# of new finds: \", num_finds - num_hits)\n",
        "    print(\"Examples of new finds:\")\n",
        "    if len(new_finds) == 0:\n",
        "      print(\"\\tNone\")\n",
        "    for tup in new_finds:\n",
        "      print('\\t', tup[1], 'is a', tup[2], '←', tup[0])\n",
        "    print('\\n---------------------------------\\n')"
      ],
      "metadata": {
        "id": "y02bvwoBVibz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Scroll through the output of the following cell."
      ],
      "metadata": {
        "id": "Sy4pAPvZDPDa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "show_results(matches, seed_tuples)"
      ],
      "metadata": {
        "id": "QFwr7EmlbklP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "💡 Think to yourself: when inspecting the new finds of each pattern, what do you notice?\n",
        "- What patterns are productive, but imprecise?\n",
        "- What patterns are precise, but unproductive?\n",
        "\n",
        "In practice, one could run multiple iterations of this bootstrapping process to gather even more patterns and animal-type tuples. Confidence thresholds can help prevent semantic drift that arises from imprecise patterns.\n",
        "\n",
        "Outside on the animal preserve, the sun is setting, and the animals are quieting down for nighttime. There's some promise here, but there's also some sense that you could refine your approach.\n",
        "\n",
        "💡 Think to yourself:\n",
        "- Is there a better way than string/phrase matching to capture patterns such as \"*This animal preserve includes __mammals__ such as __lions__, __elephants__, and __giraffes__*\"? (This [paper](https://papers.nips.cc/paper_files/paper/2004/hash/358aee4cc897452c00244351e4d91f69-Abstract.html) may contain clues.)\n",
        "- You realize that there are some sentences in the zoologists' notes that are completely missed by your pipeline, such as the second sentence here: \"*__Giraffes__ are tall. They are __mammals__.*\" What are some ways you might improve your current approach or dataset?\n",
        "- How might you extract not just ANIMAL → NOUN relations (e.g. \"kangaroo\" → \"marsupial\", \"kangaroo\" → \"macropod\"), but also NOUN → NOUN relations (e.g. \"macropod\" → \"marsupial\") to create a full taxonomy tree?"
      ],
      "metadata": {
        "id": "nvUvyKi35X2F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Closing and Submission\n",
        "\n",
        "Congratulations on finishing HW5! Please ensure that you submit this completed notebook onto Gradescope. Make sure all cells in the notebook are run so that print statements are visible.\n",
        "\n",
        "File --> Download --> Download .ipynb\n",
        "\n",
        "Make sure your notebook is named `HW5.ipynb` when you upload it to Gradescope.\n",
        "\n",
        "# Addendum\n",
        "\n",
        "The list of animals in this homework are drawn from [Wikipedia's list of animal names](https://en.wikipedia.org/wiki/List_of_animal_names), and the zoologists' notes are actually sentences pulled from Wikipedia."
      ],
      "metadata": {
        "id": "0kXb3gtU_WJp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WK2NU9Cy_eFt"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}